{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "The experiemental effort of loading single Na atom is proved to be quite challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Compare to other atoms that people have successfully trapped in a tweezer (Cs, Rb), we have identified many possible reasons including (not necessarily independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Smaller exited state hyperfine structure\n",
    "* Not as efficient sub-Doppler cooling\n",
    "* No accessible magic wavelength for D2\n",
    "* Stochastic heating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, due to the characteristic of out system (Lamb-Dicke parameter $\\eta\\approx1$), it is very hard to analytically study these effects. On the other hand, since the system is very simple, a single atom with very few degrees of freedom, it is possible to do a precise numerical study of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "1. [Quantum jump method](#Quantum-jump-method)\n",
    "\n",
    "2. [Split operator method](#Split-operator-method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Quantum jump method](https://en.wikipedia.org/wiki/Quantum_jump_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The starting point of the simulation is the master equation\n",
    "\n",
    "$$ \\small{\\dot \\rho=\\frac1{i\\hbar}[H,\\rho]+\\sum_mC_m\\rho C_m^\\dagger-\\frac12\\sum_m\\left(C_m^\\dagger C_m\\rho +\\rho C_m^\\dagger C_m\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, doing a deterministic calculation of the master equation requires the whole density matrix and many high dimensional matrix operations which is not very efficient. Instead, we use the [quantum jump method](https://en.wikipedia.org/wiki/Quantum_jump_method) (or Monte Carlo wave function (MCWF) method) to estimate the density matrix (and any measurable quantity derived from it) by evolving a wave function probabilistically.\n",
    "\n",
    "#### Rough scatch of the quantum jump method\n",
    "\n",
    "We divide the evolution into the coherent part described by the Hamiltonian $H$ and the incoherent part described by the jump operators $C_m$. In each time step of the simulation, we probabilistically choose either to evolve the wave function with the quantum jump or the Hamiltonian. The probability for doing each jump is determined by the expectation value of the jump operator $p_m=\\langle C_m^\\dagger C_m \\rangle$.\n",
    "\n",
    "* If we decide to jump, the wave function is turned into the projection of $C_m$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "jump:\n",
    "\n",
    "$$ |\\psi(t+\\delta t)\\rangle=C_m|\\psi(t)\\rangle $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* If we decide not to jump, the wave function is evolved with the original hamiltonian, plus a correction term due to the jump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "not jump:\n",
    "\n",
    "$$ H'=H-\\frac{i\\hbar}{2}\\sum_m C_m^\\dagger C_m $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Since none of these operations are unitary, the wave function needs to be normalized at each time step.\n",
    "\n",
    "Detail description of the method is available in thie master thesis http://www.oq.ulg.ac.be/master_thesis_rc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Split operator method](https://en.wikipedia.org/wiki/Split-step_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The evolution with the Hamiltonian is done with the [split operator method](https://en.wikipedia.org/wiki/Split-step_method).\n",
    "\n",
    "The Hamiltonian and the time evolution of the system are described by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ H = H_x + H_p\\qquad T=e^{iH\\delta t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "where $H_x$ and $H_p$ are only functions of $x$ and $p$ respectively.\n",
    "\n",
    "This is hard to calculate since it is not diagonal in a good basis and the diagonalization is very time consuming.\n",
    "\n",
    "So instead of directly calculation the exponential of the full Hamiltonian, we calculate the transformation $e^{iH_x\\delta t}$ and $e^{iH_p\\delta t}$ separately on the wave function. This is easy to do in the $x$ and $p$ basis and the tranformation of basis can be done very efficiently with [fast Fourier transformation](https://en.wikipedia.org/wiki/Fast_Fourier_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Propagate with $H_x$,\n",
    "\n",
    "$$ \\psi'(t, x) = e^{iH_x\\delta t} \\psi(t, x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go to $p$ basis and propagate with $H_p$\n",
    "\n",
    "$$ \\tilde\\psi'(t, p) = e^{iH_p\\delta t} FFT\\{\\psi'(t, x)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go back to $x$ basis\n",
    "\n",
    "$$ \\psi(t + \\delta t, x) \\approx IFFT\\{\\tilde\\psi'(t, p)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance optimization\n",
    "\n",
    "1. [Task parallelism](#Task-parallelism)\n",
    "\n",
    "2. [Reduce memory allocation](#Reduce-memory-allocation)\n",
    "\n",
    "3. [Memory locality](#Memory-locality)\n",
    "    \n",
    "4. [SIMD (Single instruction multiple data) (a.k.a. Data parallelism)][1]\n",
    "\n",
    "5. [Subnormal numbers](#Subnormal-numbers)\n",
    "\n",
    "\n",
    "[1]: #SIMD-(Single-instruction-multiple-data)-(a.k.a.-Data-parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### [Task parallelism](https://en.wikipedia.org/wiki/Task_parallelism)\n",
    "\n",
    "Since each time evolution in a Monte-Carlo simulation and each Monte-Carlo simulation in a multi-parameter scan are all independent, this is a embarrassingly parallelizable problem and can be done very efficiently with multiple processes (until the initialization cost kicks in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Reduce memory allocation\n",
    "\n",
    "Memory allocations are expensive (to be improved). Reused buffers, use inplace operation, avoid boxing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Memory locality](https://en.wikipedia.org/wiki/Locality_of_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The CPU is several orders of magnitude faster than the memory. Multiple levels of CPU Cache are introduced to hide this latency but they are much smaller than the main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Iterate over multi dimension array\n",
    "\n",
    "    This usually means the inner loop should iterate over the continuous dimension of an array so it is important to know how array are stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For example, in languages using row-major format (`C`/`C++`, `Mathematica`, `Python`) the preferred way to interate over an multi dimentional array is,\n",
    "\n",
    "```c++\n",
    "\n",
    "for (int i = 0;i < n1;i++) {\n",
    "    for (int j = 0;j < n2;j++) {\n",
    "        A[i][j] = B[i][j] + C[i][j];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "in languages using column-major format (`MATLAB`, `R`, `Julia`), the preferred way is\n",
    "\n",
    "```julia\n",
    "\n",
    "for i in 1:n1\n",
    "    for j in 1:n2\n",
    "        A[j, i] = B[j, i] + C[j, i]\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "If it is necessary to iterate over both dimensions simultaneously (e.g. for matrix multiplication) it is sometimes beneficial to divide the array in to blocks to improve memory locality. See also [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference#Matrix_multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Predictable memory access patterns also help CPU prefetching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are more to consider for multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [SIMD (Single instruction multiple data)](https://en.wikipedia.org/wiki/SIMD) (a.k.a. [Data parallelism](https://en.wikipedia.org/wiki/Data_parallelism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Modern processors introduce instructions that can process multiple data at a time to speed up CPU bound data processing. Most popular ones on the x86 processors are the [SSE family](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions) and the more recent [AVX family](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions). ARM processors also introduce [Advanced SIMD (NEON)](https://en.wikipedia.org/wiki/ARM_architecture#NEON) since ARMv6.\n",
    "\n",
    "It is not always necessary to write assembly manually in order to exploit this function. The compiler can do the transformation automatically in many cases (in julia this is done using the [`@simd` macro](http://julia.readthedocs.org/en/latest/manual/performance-tips/?highlight=simd#performance-annotations)). However, this automatic optimization has some limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* No control flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This means no branches and no function calls. Simple branches can be replaced with branchless [`ifelse` function call](http://julia.readthedocs.org/en/latest/stdlib/base/?highlight=ifelse#Base.ifelse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Strided load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Most SIMD instruction set only support loading continious block of memory (`AVX2` and `NEON` might have support for strided/structural load). This is not a problem for scalar types if the iteration is along a continious direction. However, for composite type, e.g. complex number, if they are stored as an array of structures (AoS), a better way to store them is structure of arrays (SoA). To see this, a complex AoS is stored in memory as\n",
    "\n",
    "```\n",
    "AoS complex: [r1][i1][r2][i2][r2][i2][r2][i2]...\n",
    "```\n",
    "\n",
    "while a complex SoA is stored as\n",
    "\n",
    "```\n",
    "SoA complex:\n",
    "    real_ary: [r1][r2][r3][r4]...\n",
    "    imag_ary: [i1][i2][i3][i4]...\n",
    "```\n",
    "\n",
    "Since the real and imaginary part of the complex number will likely be doing very different operations in each iteration while each of them are doing similar things between iteration, the vectorized version of this will operate on a vector of the real part and a vector of the imaginary part, i.e.\n",
    "\n",
    "```\n",
    "real_vec: [r1][r2][r3][r4]\n",
    "imag_vec: [i1][i2][i3][i4]\n",
    "```\n",
    "\n",
    "As one can easily see, this is not the natural layout for AoS but is very natual for SoA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data dependency and [alias analysis](https://en.wikipedia.org/wiki/Alias_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another limitation of the SIMD optimization is the there shouldn't be any data dependency between different loop iterations, e.g. the following loop cannot easily be vectorized,\n",
    "\n",
    "```julia\n",
    "\n",
    "for i in 1:(n - 1)\n",
    "    A[i + 1] = A[i] * 2\n",
    "end\n",
    "```\n",
    "\n",
    "Sometimes, there can be false positive data dependency since the compiler cannot proof that certain memory operations are independent. e.g. if `struct` is an arbitrary mutable type and `field` is one of its field, the compile may not vectorize the following loop because it cannot prove that the address of `A` and `struct` are independent (i.e. they don't alias).\n",
    "\n",
    "```julia\n",
    "\n",
    "for i in 1:n\n",
    "    A[i] *= struct.field\n",
    "end\n",
    "```\n",
    "\n",
    "This can usually be solved by manually hoisting the load out of the loop, i.e. the compiler should be able to vectorize the following loop.\n",
    "\n",
    "```julia\n",
    "\n",
    "v = struct.field\n",
    "for i in 1:n\n",
    "    A[i] *= v\n",
    "end\n",
    "```\n",
    "\n",
    "If the alias analysis cannot prove that `struct` and `A` are independent, manual load hoisting can improve performance even if the loop cannot be vectorized since it reduce memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Make the loop smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The CPU only have limited logical SIMD registers (16x for sse4 and avx2, 32x for avx512) and if there are more temporary result than the number of registers, some results will be spilled to the stack. This cause memory access that can degrade the performance by a lot even though it will almost certainly be in the L1 cache.\n",
    "\n",
    "Reducing the loop size also help reducing the decoding cost on x86 CPU. Intel introduces micro-ops caches that can optimize instruction decoding in tight loops. The cache can hold hundreds or a thousand instructions depending on the generation and model. If the loop does not fit in the micro-op cache, the program is very likely instruction decoding bounded. (likely to hit register limit first in real programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### [Subnormal number](https://en.wikipedia.org/wiki/Denormal_number)\n",
    "\n",
    "Subnormal numbers (or denormal numbers) are floating point numbers that are too small to be normalized in the binary representation (the exponant underflows). On certain CPUs (most x86), handling of subnormal number is done in software using micro-code. This makes them much slower than other floating point values (10x or more).\n",
    "\n",
    "It is possible to truncate subnormal numbers to zero if it safe to do so. On x86 CPU using sse instructions, this is done by setting certain bits in the MXCSR register. Julia provides `get/set_zero_subnormals` to query and set these bits. It is also possible to avoid this problem by introducing noise. See also [Treat subnormal numbers as zeros](http://julia.readthedocs.org/en/latest/manual/performance-tips/#treat-subnormal-numbers-as-zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current status and preliminary results\n",
    "\n",
    "* [Resolved sidebandcooling](#Resolved-sideband-cooling)\n",
    "* [Doppler cooling](#Doppler-cooling)\n",
    "* [Stochastic heating](#Stochastic-heating)\n",
    "* [Escape and photon count](#Escape-and-photon-count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resolved sideband cooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Line width narrower than the trapping frequency\n",
    "* Drive in resonance with sideband (detuning equal to trapping frequency)\n",
    "* Weak drive (Rabi frequency comparable to line width)\n",
    "* Start off-center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Sideband cooling wave function](../results/sideband/na_sideband_cool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Temperatures\n",
    "\n",
    "![Sideband temperature](../results/sideband/na_sideband_cool-temp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Doppler cooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Line width wider than the trapping frequency\n",
    "* Large detuning (comparable to line width)\n",
    "* Weak drive (Rabi frequency smaller than to detuning)\n",
    "* Start off-center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Doppler cooling temperature](../results/doppler/na_doppler_cool_magic-temp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stochastic heating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Same with Doppler cooling but the excited state trapping potential can be different from the ground state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Temperature vs Excited state potential\n",
    "\n",
    "($-0.5\\Gamma=-5MHz$ detuning)\n",
    "\n",
    "![5MHz](../results/stochastic_heat/ext_trap-vs-temp_det-5^Rabi1_center.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Temperature vs Excited state potential\n",
    "\n",
    "($-3\\Gamma=-30MHz$ detuning)\n",
    "\n",
    "![30MHz](../results/stochastic_heat/ext_trap-vs-temp_det-30^Rabi2_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The stochastic heating problem is not too bad if the excited state is more trapping then the ground state. However, it gets worse much faster if the excited state trapping potential is weaker than the ground state.\n",
    "\n",
    "Increasing the (red) detuning helps, which might be what saved rubidium people. However, this is not really an option for sodium on the D2 line because of the small hyperfine structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Escape and photon count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Add threshold to represent atom leaving the trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](../results/photon_count/escape^time_vs_det-Rabi2^two_beams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../results/photon_count/pcount_vs_det-Rabi2^two_beams.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.6.0-pre.beta",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
